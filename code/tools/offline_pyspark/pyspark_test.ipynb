{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_PYTHON']='./xxx/xxx/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def spark_init(app_name, queue_name='xxx'):\n",
    "    # 设置sparkSession配置参数\n",
    "    # 通过SparkConf设置好启动spark应用的所有参数\n",
    "    conf = SparkConf()\n",
    "    ## spark应用参数\n",
    "    conf.set('spark.master', 'yarn') # 集群管理方式，此处可选择是是使用yarn-client模式启动spark，还是使用local模式\n",
    "    conf.set('spark.submit.deployMode', 'client')\n",
    "    conf.set('spark.app.name', app_name) # spark应用名称\n",
    "    # conf.set('spark.driver.cores', '10') # dirver使用的核心数\n",
    "    conf.set('spark.driver.memory', '12g') # driver使用的内存大小，spark应用启动后不可设置该参数，这个参数很有必要，防止容器内存溢出导致进程被杀死\n",
    "    conf.set('spark.executor.memory', '12g') # executor使用的内存大小\n",
    "    # conf.set('spark.executor.cores', '3') # executor使用的核心数\n",
    "    \n",
    "    # 集群python环境\n",
    "    archives = 'hdfs://xxx.tar.gz#xxx'\n",
    "    conf.set('spark.yarn.dist.archives', archives)\n",
    "    conf.set('spark.pyspark.driver.python', 'xxx/bin/python') # driver端本地python路径\n",
    "    conf.set('spark.pyspark.python', './xxx/xxx/bin/python') # executor端python路径\n",
    "    conf.set('spark.driver.host', 'xxx.xxx.xxx.xxx')\n",
    "    \n",
    "    # 动态资源分配\n",
    "    # conf.set('spark.shuffle.service.enabled', 'true')\n",
    "    conf.set('spark.dynamicAllocation.enabled', 'true')\n",
    "    conf.set('spark.dynamicAllocation.maxExecutors', '500')\n",
    "    conf.set('spark.dynamicAllocation.minExecutors', '100') # 该值建议设置小一点，过大会影响组内Hadoop资源\n",
    "    conf.set('spark.task.cpus', '6')\n",
    "    \n",
    "    # 读取文件格式不需要orc格式\n",
    "    conf.set('spark.sql.hive.convertMetastoreOrc', 'false')\n",
    "\n",
    "    # 设置队列与hive\n",
    "    conf.set('spark.yarn.queue', queue_name)\n",
    "    # conf.set('mapreduce.input.fileinputformat.input.dir.recursive', 'true')\n",
    "    \n",
    "    # 初始化sparksession\n",
    "    spark = (SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate())\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    return spark, sc\n",
    "\n",
    "spark, sc = spark_init('uplift_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型\n",
    "import joblib\n",
    "import causalml\n",
    "\n",
    "\n",
    "learner_x_lgb = joblib.load('xxx.model')\n",
    "learner_x_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型广播\n",
    "bc_learner_x_lgb = sc.broadcast(learner_x_lgb)\n",
    "bc_learner_x_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 特征数据读取\n",
    "str_sql = '''\n",
    "select \n",
    "    * \n",
    "from \n",
    "    xxx \n",
    "where \n",
    "    concat_ws('-', year, month, day) = '2022-02-06' \n",
    "limit 20\n",
    "'''\n",
    "\n",
    "df_test = spark.sql(str_sql)\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = list(df_test.columns)\n",
    "print(len(features))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(features[1:-3]))\n",
    "features[1:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征广播\n",
    "bc_feats = sc.broadcast(features[1:-3])\n",
    "bc_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型字典广播\n",
    "import numpy as np\n",
    "dict_p = {\n",
    "    'treatment_1': np.array([0.0653]), \n",
    "    'treatment_2': np.array([0.2908]), \n",
    "    'treatment_3': np.array([0.0653]), \n",
    "    'treatment_4': np.array([0.0476]), \n",
    "    'treatment_5': np.array([0.0367])\n",
    "}\n",
    "bc_dict_p = sc.broadcast(dict_p)\n",
    "bc_dict_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测udf\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 单行输入，单值输出\n",
    "# @F.udf(returnType=FloatType())\n",
    "# def predictor(*list_feature):\n",
    "#     dict_p = {\n",
    "#         'treatment_1': np.array([0.0653]), \n",
    "#         'treatment_2': np.array([0.2908]), \n",
    "#         'treatment_3': np.array([0.0653]), \n",
    "#         'treatment_4': np.array([0.0476]), \n",
    "#         'treatment_5': np.array([0.0367])\n",
    "#     }\n",
    "    \n",
    "#     return float(bc_learner_x_lgb.value.predict(np.array(list_feature).reshape(1, -1), p=dict_p)[:, 1])\n",
    "\n",
    "\n",
    "# 单行输入，多值转Array输出\n",
    "# @F.udf(returnType=ArrayType(FloatType()))\n",
    "# def predictor(*list_feature):\n",
    "#     dict_p = {\n",
    "#         'treatment_1': np.array([0.0653]), \n",
    "#         'treatment_2': np.array([0.2908]), \n",
    "#         'treatment_3': np.array([0.0653]), \n",
    "#         'treatment_4': np.array([0.0476]), \n",
    "#         'treatment_5': np.array([0.0367])\n",
    "#     }\n",
    "    \n",
    "#     score = bc_learner_x_lgb.value.predict(np.array(list_feature).reshape(1, -1), p=dict_p)\n",
    "#     score_coupon_20 = float(score[:, 0])\n",
    "#     score_coupon_50 = float(score[:, 1])\n",
    "#     score_coupon_100 = float(score[:, 2])\n",
    "#     score_coupon_150 = float(score[:, 3])\n",
    "#     score_coupon_180 = float(score[:, 4])\n",
    "    \n",
    "#     return (score_coupon_20, score_coupon_50, score_coupon_100, score_coupon_150, score_coupon_180)\n",
    "\n",
    "\n",
    "def prdeictBatch(datas, feats, model, p_score):\n",
    "    for data in datas:\n",
    "        tmp = np.array(list(data)).reshape(-1, 1+len(feats.value)+3)[:, 1:-3].astype(float)\n",
    "        score = model.value.predict(tmp, p=p_score.value)\n",
    "        score_coupon_20 = float(score[:, 0])\n",
    "        score_coupon_50 = float(score[:, 1])\n",
    "        score_coupon_100 = float(score[:, 2])\n",
    "        score_coupon_150 = float(score[:, 3])\n",
    "        score_coupon_180 = float(score[:, 4])\n",
    "        try:\n",
    "            yield (int(data['uid']), score_coupon_20, score_coupon_50, score_coupon_100, score_coupon_150, score_coupon_180)\n",
    "        except StopIteration:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_test_pred = df_test.withColumn('prediction', predictor(*features[1:-3]))\n",
    "rdd_pred = df_test.repartition(10).rdd.mapPartitions(lambda x: prdeictBatch(x, bc_feats, bc_learner_x_lgb, bc_dict_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd_pred.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rdd_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_test_pred_res = df_test_pred.select('uid', 'prediction')\n",
    "df_pred = spark.createDataFrame(rdd_pred, ['uid', 'score_coupon_20', 'score_coupon_50', 'score_coupon_100', 'score_coupon_150', 'score_coupon_180'])\n",
    "# df_pred = rdd_pred.toDF(['uid', 'score_coupon_20', 'score_coupon_50', 'score_coupon_100', 'score_coupon_150', 'score_coupon_180'])\n",
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_pred_res_flatten = df_test_pred_res.selectExpr('uid', \n",
    "                                                       'prediction[0] as score_coupon_20', \n",
    "                                                       'prediction[1] as score_coupon_50', \n",
    "                                                       'prediction[2] as score_coupon_100', \n",
    "                                                       'prediction[3] as score_coupon_150', \n",
    "                                                       'prediction[4] as score_coupon_180')\n",
    "df_test_pred_res_flatten.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test_pred_res_flatten.cache()\n",
    "df_test_pred_res_flatten.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pred_res_flatten.createOrReplaceTempView('table_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 建表\n",
    "# 只运行一次\n",
    "str_sql_create_table = '''\n",
    "create table if not exists xxx \n",
    "(\n",
    "    uid bigint comment '用户id', \n",
    "    score_coupon_20 double comment '增益打分（0.2）', \n",
    "    score_coupon_50 double comment '增益打分（0.5）', \n",
    "    score_coupon_100 double comment '增益打分（1.0）', \n",
    "    score_coupon_150 double comment '增益打分（1.5）', \n",
    "    score_coupon_180 double comment '增益打分（1.8）'\n",
    ")\n",
    "comment 'xxx' \n",
    "partitioned by\n",
    "(\n",
    "    year string comment '年', \n",
    "    month string comment '月', \n",
    "    day string comment '日'\n",
    ")\n",
    "'''\n",
    "\n",
    "spark.sql(str_sql_create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 写表\n",
    "str_sql_write = '''\n",
    "insert \n",
    "    overwrite table \n",
    "        xxx \n",
    "    partition \n",
    "    (\n",
    "        year='2022', \n",
    "        month ='02', \n",
    "        day='06'\n",
    "    ) \n",
    "select \n",
    "    uid, \n",
    "    score_coupon_20, \n",
    "    score_coupon_50, \n",
    "    score_coupon_100, \n",
    "    score_coupon_150, \n",
    "    score_coupon_180 \n",
    "from \n",
    "    table_tmp \n",
    "'''\n",
    "\n",
    "spark.sql(str_sql_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 关闭连接\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (jarretthan)",
   "language": "python",
   "name": "jarretthan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
